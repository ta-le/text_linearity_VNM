{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Data Preprocessing\n",
    " - **INPUT**: Tokenized/segmented texts (using VnCoreNLP in JAVA) of 2 classes: _TECH_ and _NON-TECH_\n",
    " - **STEPs**:\n",
    "   - Remove digits\n",
    "   - Remove stopwords\n",
    "   - Remove special/\"weird\" patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenized texts and put into a dataframe\n",
    "from utilities import read_file\n",
    "import os\n",
    "\n",
    "\n",
    "# get file names in TECH folder\n",
    "indir = 'data/tokenized/TECH/'\n",
    "file_names = os.listdir(indir)\n",
    "\n",
    "tech_texts = list()\n",
    "for file_name in file_names:\n",
    "    file = os.path.join(indir, file_name)\n",
    "    tech_texts += [read_file(file)]\n",
    "\n",
    "\n",
    "# get file names in NON-TECH folder\n",
    "indir = 'data/tokenized/NON-TECH/'\n",
    "file_names = os.listdir(indir)\n",
    "\n",
    "non_tech_texts = list()\n",
    "for file_name in file_names:\n",
    "    file = os.path.join(indir, file_name)\n",
    "    non_tech_texts += [read_file(file)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Define stopwords list using \"vietnamese-stopwords-dash.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'vietnamese-stopwords-dash.txt'\n",
    "## consider puntuations and one-character words as stopwords to remove\n",
    "stopwords = list('!@#$%^&*()_+-=<>?,./:\\'''“”;abcdefghijklmnopqrstuvwxyz')\n",
    "stopwords = (open(filename)).read().split('\\n') + stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove digits, weird-patterns & stopwords, using an outside function in **utilities.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import preprocess_tokenized_text\n",
    "\n",
    "tech_tokens = list(map(\n",
    "    lambda tokenized_text:preprocess_tokenized_text(tokenized_text,\n",
    "                                                    stopwords),\n",
    "    tech_texts[:]))\n",
    "\n",
    "\n",
    "non_tech_tokens = list(map(\n",
    "    lambda tokenized_text:preprocess_tokenized_text(tokenized_text,\n",
    "                                                    stopwords),\n",
    "    non_tech_texts[:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare a classification dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df_tech = pd.DataFrame({\n",
    "    \"text\": tech_texts,\n",
    "    'token': tech_tokens,\n",
    "    'target': np.ones_like(tech_tokens)\n",
    "})\n",
    "\n",
    "df_non_tech = pd.DataFrame({\n",
    "    \"text\": non_tech_texts,\n",
    "    'token': non_tech_tokens,\n",
    "    'target': np.zeros_like(non_tech_tokens)\n",
    "})\n",
    "\n",
    "df = df_tech.append(df_non_tech, ignore_index=True)\n",
    "\n",
    "\n",
    "# save file\n",
    "from utilities import save_pkl\n",
    "outfile = 'data/clf_data.pkl'\n",
    "save_pkl(df, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = np.array(df.target)\n",
    "X = np.array(df.loc[:, ['text','token']])\n",
    "\n",
    "# split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train data\n",
    "df_train = pd.DataFrame(columns=['target', 'text', 'tokens'])\n",
    "df_train.target = y_train\n",
    "df_train.text = X_train[:,0]\n",
    "df_train.tokens = X_train[:,1]\n",
    "\n",
    "outfile = 'data/data_train.pkl'\n",
    "save_pkl(df_train, outfile)\n",
    "\n",
    "\n",
    "# save test data\n",
    "df_test = pd.DataFrame(columns=['target', 'text', 'tokens'])\n",
    "df_test.target = y_test\n",
    "df_test.text = X_test[:,0]\n",
    "df_test.tokens = X_test[:,1]\n",
    "\n",
    "outfile = 'data/data_test.pkl'\n",
    "save_pkl(df_test, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
